# 4.设计

## 4.1 动机
我们设计`Kafka`使其能够作为一个统一的平台来处理大公司可能拥有的所有实时数据馈送。为此，我们必须考虑一组相当广泛的用例。

它必须具有高吞吐量才能支持大量事件流，例如实时日志聚合。

它需要优雅地处理大量数据积压，以支持从脱机系统定期加载数据。

这也意味着系统将必须处理低延迟传递，以处理更多传统的消息传递用例。

我们希望支持对这些提要进行分区，分布式，实时处理，以创建新的派生提要。这激发了我们的分区和消费者模型。

最终，在将流馈送到其他数据系统进行服务的情况下，我们知道该系统必须能够在存在机器故障的情况下保证容错能力。

支持这些用途使我们设计出了许多独特的元素，与传统的消息传递系统相比，它更类似于数据库日志。在以下各节中，我们将概述设计的某些元素。

## 4.2 持久性

### 不要害怕文件系统！
`Kafka`严重依赖文件系统来存储和缓存消息。人们普遍认为“磁盘速度很慢”，这使人们怀疑持久性结构能否提供有竞争力的性能。实际上，磁盘的使用速度和速度都比人们期望的要慢得多和快得多。正确设计的磁盘结构通常可以和网络一样快。

有关磁盘性能的关键事实是，过去十年来，硬盘驱动器的吞吐量与磁盘寻道的等待时间有所不同。结果，在具有六个`7200rpm SATA RAID-5`阵列配置的[`JBOD`](https://en.wikipedia.org/wiki/Non-RAID_drive_architectures)上，线性写入的性能约为`600MB/sec`，但随机写入的性能仅为`100k/sec`，相差超过6000倍。这些线性读取和写入是所有使用模式中最可预测的，并且已由操作系统进行了大幅优化。现代操作系统提供了预读和后写技术，这些技术可以以大块倍数预取数据，并将较小的逻辑写入分组为大型物理写入。可以在此[ACM队列文章](https://queue.acm.org/detail.cfm?id=1563874)中找到对该问题的进一步讨论；他们实际上发现 [在某些情况下，顺序磁盘访问可能比随机内存访问快！](https://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)

为了弥补这种性能差异，现代操作系统在使用主内存进行磁盘缓存方面变得越来越积极。当回收内存时，现代的操作系统会很乐意将所有可用内存转移到磁盘缓存中，而对性能的影响很小。所有磁盘读写都将通过此统一缓存。如果不使用直接`I/O`，则无法轻松关闭此功能，因此，即使进程维护了数据的进程内高速缓存，此数据也很可能在`OS`页面高速缓存中重复，从而有效地将所有数据存储两次。

此外，我们是在`JVM`之上构建的，任何花时间使用`Java`内存的人都知道两件事：

1. 对象的内存开销非常高，通常使存储的数据大小增加一倍（或更糟）。
2. 随着堆内数据的增加，`Java`垃圾收集变得越来越轻而易举。

由于这些因素的影响，使用文件系统并依靠页面缓存优于维护内存中缓存或其他结构-通过自动访问所有可用内存，我们至少使可用缓存增加了一倍，并且通过存储紧凑的存储空间，我们有可能再次将其增加一倍字节结构而不是单个对象。这样做将导致在`32GB`的计算机上最多可缓存`28-30GB`的缓存，而不会造成`GC`损失。此外，即使重新启动服务，此高速缓存也将保持温暖，而进程内高速缓存将需要在内存中重建（对于`10GB`的高速缓存可能需要10分钟），否则它将需要从完全冷的高速缓存开始（这可能意味着糟糕的初始性能）。这也大大简化了代码，因为所有用于维护缓存和文件系统之间一致性的逻辑现在都在`OS`中，与一次性进行中的尝试相比，这样做往往更有效，更正确。如果您的磁盘使用情况支持线性读取，那么预读将在每次读取的磁盘上有效地预先填充有用的数据。

这表明设计非常简单：当我们空间不足时，不要在内存中维护尽可能多的内存并将其全部刷新到文件系统中，而是将其反转。所有数据都会立即写入文件系统上的持久日志中，而不必刷新到磁盘。实际上，这仅意味着将其传输到内核的页面缓存中。

这种样式的以页面缓存为中心的设计风格在此处有关`Varnish`设计的[文章](http://varnish-cache.org/docs/trunk/phk/notes.html)中进行了描述（以及一些自大的技巧）。

### 恒定时间就足够了
消息传递系统中使用的持久性数据结构通常是每个用户队列，具有关联的`BTree`或其他通用的随机访问数据结构，以维护有关消息的元数据。`BTree`是可用的最通用的数据结构，它使在消息传递系统中支持各种事务性和非事务性语义成为可能。但是，它们的代价确实很高：`Btree`操作是`O(logN)`。通常，`O(logN)`本质上等同于恒定时间，但是对于磁盘操作而言并非如此。磁盘搜寻的发生时间为10毫秒，每个磁盘一次只能执行一次搜寻，因此并行性受到限制。因此，即使很少的磁盘搜寻也会导致非常高的开销。由于存储系统将非常快的缓存操作与非常慢的物理磁盘操作混合在一起，

直观上讲，持久性队列可以建立在简单的读取上并附加到文件上，这与日志记录解决方案通常是一样的。这种结构的优点是所有操作均为`O(1)`，读取不会阻塞写入或彼此阻塞。这具有明显的性能优势，因为性能与数据大小完全脱钩-一台服务器现在可以充分利用许多廉价的低转速1 + TB SATA驱动器。尽管它们的寻道性能较差，但这些驱动器对于大型读写具有可接受的性能，价格仅为价格的1/3和容量的3倍。

可以访问几乎无限的磁盘空间而不会降低性能，这意味着我们可以提供消息传递系统中通常不具备的某些功能。例如，在`Kafka`中，我们可以将消息保留相对较长的时间段（例如一周），而不是尝试在消息被使用后立即删除。正如我们将要描述的，这为消费者带来了很大的灵活性。

## 4.3 效率
我们为提高效率付出了巨大的努力。我们的主要用例之一是处理非常大的`Web`活动数据：每个页面视图可能会产生数十次写入。此外，我们假设发布的每条消息都至少由一个消费者（通常是许多消费者）阅读，因此我们努力使消费尽可能便宜。

我们还从建立和运行许多类似系统的经验中发现，效率是有效的多租户操作的关键。如果下游基础设施服务由于应用程序使用量的小幅度增加而容易成为瓶颈，那么这种小的更改通常会带来问题。速度非常快，我们可以帮助确保应用程序在基础架构之前先在负载下翻转。当尝试运行支持集中式集群上数十个或数百个应用程序的集中式服务时，这尤其重要，因为使用模式的变化几乎每天都会发生。

我们在上一节中讨论了磁盘效率。一旦消除了不良的磁盘访问模式，这种系统效率低下的常见原因有两个：过多的小`I/O`操作和过多的字节复制。

小型`I/O`问题在客户端和服务器之间以及服务器自身的持久性操作中均会发生。

为避免这种情况，我们的协议围绕“消息集”抽象构建，该抽象将消息自然地组合在一起。这允许网络请求将消息分组在一起并分摊网络往返的开销，而不是一次发送单个消息。服务器又将消息块一次性添加到其日志中，而使用者则一次获取大型线性块。

这种简单的优化可以加快数量级的速度。批处理导致更大的网络数据包，更大的顺序磁盘操作，连续的内存块等，所有这些都使`Kafk`a能够将突发的随机消息写入流转换为线性写入流，这些写入流将流向使用者。

另一个低效率是字节复制。在低消息速率下，这不是问题，但是在负载下影响很大。为了避免这种情况，我们采用了标准的二进制消息格式，该格式由生产者，代理和消费者共享（因此，数据块可以在它们之间进行修改而无需修改）。

代理维护的消息日志本身只是文件目录，每个文件目录都由一系列消息集填充，这些消息集以生产者和使用者所用的相同格式写入磁盘。维持这种通用格式可以优化最重要的操作：持久日志块的网络传输。现代的`Unix`操作系统提供了高度优化的代码路径，用于将数据从页面缓存传输到套接字。在`Linux`中，这是通过`sendfile`系统调用完成的。

要了解`sendfile`的影响，重要的是要了解将数据从文件传输到套接字的通用数据路径：

1. 操作系统将数据从磁盘读取到内核空间中的页面缓存中
2. 应用程序将数据从内核空间读取到用户空间缓冲区中
3. 应用程序将数据写回到内核空间的套接字缓冲区中
4. 操作系统将数据从套接字缓冲区复制到通过网络发送的`NIC`缓冲区

这显然是低效的，有四个副本和两个系统调用。使用`sendfile`，可以通过允许`OS`将数据从页面缓存直接发送到网络来避免这种重新复制。因此，在此优化路径中，仅需要最终复制到`NIC`缓冲区。

我们希望一个常见的用例是某个主题的多个使用者。使用上述零复制优化，数据将被复制到页面缓存中一次，并在每次使用时重复使用，而不是存储在内存中，并在每次读取时复制到用户空间。这允许以接近网络连接限制的速率消耗消息。

页面缓存和`sendfile`的这种组合意味着，在大多数用户被卡住的`Kafka`群集上，您将看不到磁盘上的任何读取活动，因为它们将完全从缓存中提供数据。

有关`Java`中`sendfile`和零复制支持的更多背景信息，请参阅[本文](https://developer.ibm.com/articles/j-zerocopy/)。

### 端到端批量压缩
在某些情况下，瓶颈实际上不是`CPU`或磁盘，而是网络带宽。对于需要通过广域网在数据中心之间发送消息的数据管道而言，尤其如此。当然，用户总是可以一次压缩其消息，而无需`Kafka`的任何支持，但这可能导致非常糟糕的压缩率，因为大量冗余是由于相同类型消息之间的重复（例如， `Web`日志中的`JSON`或用户代理或通用字符串值）。高效压缩需要将多个消息压缩在一起，而不是分别压缩每个消息。

`Kafka`以有效的批处理格式支持此操作。一批消息可以压缩在一起，然后以这种形式发送到服务器。这批消息将以压缩形式写入，并保持压缩在日志中，并且仅由使用者解压缩。

`Kafka`支持`GZIP`，`Snappy`，`LZ4`和`ZStandard`压缩协议。有关压缩的更多详细信息，请参见[此处](https://cwiki.apache.org/confluence/display/KAFKA/Compression)。

## 4.4 生产者
### 负载均衡
生产者将数据直接发送到作为分区领导者的代理，而无需任何中间路由层。为了帮助生产者做到这一点，所有`Kafka`节点都可以在任何给定时间回答关于元数据的请求，这些数据关于哪些服务器处于活动状态以及主题分区的领导者在哪里，以允许生产者适当地定向其请求。

客户端控制将消息发布到哪个分区。这可以随机执行，实现一种随机负载平衡，也可以通过某些语义分区功能来完成。我们通过允许用户指定要用来进行分区的键并使用它来哈希到分区来公开用于语义分区的接口（如果需要，还可以选择覆盖分区功能）。例如，如果选择的键是用户`ID`，则给定用户的所有数据将发送到同一分区。反过来，这将允许消费者对他们的消费做出所在地假设。明确设计了这种分区样式，以允许使用方中进行本地敏感的处理。

### 异步发送
批处理是提高效率的主要驱动力之一，而要启用批处理，`Kafka`生产者将尝试在内存中累积数据并在单个请求中发出更大的批处理。批处理可以配置为累积不超过固定数量的消息，并且等待不超过某个固定等待时间限制（例如64k或10ms）。这允许累积更多的字节来发送，并且服务器上很少进行较大的`I/O`操作。此缓冲是可配置的，并提供了一种机制来权衡少量额外的延迟以提高吞吐量。

有关配置和生产者[`api`](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)的详细信息，可以在文档的其他地方找到。

## 4.5 消费者
`Kafka`使用者通过向负责它要使用的分区的代理发出“获取”请求来工作。使用者通过每个请求在日志中指定其偏移量，并从该位置开始接收回日志的一部分。消费者因此对该位置具有重要的控制权，并且可以根据需要倒带以重新使用数据。

### 推与拉
我们考虑的第一个问题是，消费者应该从`broker`那里提取数据还是`broker`应该将数据推送给消费者。在这方面，`Kafka`遵循了一种更为传统的设计，大多数消息传递系统都采用这种设计，在这种设计中，数据被从生产者推送到代理，并被消费者从代理那里拉出。一些以日志为中心的系统，例如`Scribe`和`Apache Flume`，则遵循完全不同的基于推送的路径，在该路径中数据被向下推送。两种方法都各有利弊。但是，基于推送的系统很难与多样化的消费者打交道，因为`broker`控制着数据传输的速率。通常，目标是使消费者能够以最大可能的速度消费。不幸的是，在推式系统中，这意味着当消费者的消费率低于生产率时，消费者往往不知所措（本质上是拒绝服务攻击）。基于拉动的系统具有更好的属性，即消费者可以轻易落后并赶上它。可以通过某种退避协议来减轻这种情况，消费者可以通过该退避协议表明其不堪重负，但是要使转移率充分利用（但绝不要过度利用）消费者，就比看起来棘手的多。先前以这种方式构建系统的尝试使我们选择了更传统的拉动模型。

基于拉取的系统的另一个优点是，它有助于对发送给使用者的数据进行积极的批处理。基于推送的系统必须选择立即发送请求或累积更多数据，然后在不知道下游使用者是否能够立即处理请求的情况下稍后发送。如果针对低延迟进行了调整，这将导致每次仅发送一条消息，以使传输最终无论如何都被缓冲，这很浪费。基于拉取的设计可解决此问题，因为使用者始终将所有可用消息拉至其在日志中的当前位置之后（或达到某些可配置的最大大小）。这样一来，您可以在不引入不必要延迟的情况下获得最佳批处理。

基于天真拉动的系统的不足之处在于，如果代理没有数据，则消费者可能会在紧密的循环中进行轮询，从而实际上忙于等待数据到达。为避免这种情况，我们在拉取请求中有一些参数，这些参数允许使用者请求阻塞在“长时间轮询”中，直到数据到达为止（并可选地等待直到给定数量的字节可用以确保较大的传输大小）。

您可以想像其他可能的设计，只是端到端的拉动。生产者将在本地写入本地日志，`broker`将从本地日志中提取，而消费者则从他们那里提取日志。经常提出类似类型的“存储转发”生产者。这很有趣，但是我们觉得它不适合我们的目标用例，因为它有成千上万的生产者。我们在大规模运行持久性数据系统方面的经验使我们感到，涉及许多应用程序的系统中成千上万个磁盘实际上并不能使事情变得更可靠，并且将是一场噩梦。在实践中，我们发现我们可以大规模运行具有强大SLA的管道，而无需生产者持久性。

### 消费者立场
跟踪的东西已经被消耗的，奇怪的是，一个邮件系统的关键性能点之一。
大多数消息传递系统保留有关在代理上消耗了哪些消息的元数据。即，当消息被发送给消费者时，`broker`要么立即在本地记录该事实，要么可以等待消费者的确认。这是一个相当直观的选择，实际上对于单台机器服务器，尚不清楚该状态还可以到达何处。由于许多消息传递系统中用于存储的数据结构伸缩性很差，因此这也是一个务实的选择-因为代理知道所消耗的内容，因此可以立即删除它，从而使数据量变小。

可能并不明显的是，使`broker`和消费者就已消费的商品达成协议并不是一个小问题。如果代理每次将消息记录到网络上立即将其消耗掉，则如果使用者无法处理该消息（例如，由于崩溃或请求超时或其他原因），则该消息将丢失。为了解决这个问题，许多消息传递系统都添加了确认功能，这意味着消息仅被标记为已发送，发送时不被使用。`broker`等待消费者的特定确认，将消息记录为已使用。此策略解决了丢失消息的问题，但会带来新的问题。首先，如果使用者处理该消息，但是在发送确认之前失败，则该消息将被使用两次。第二个问题与性能有关，现在代理必须对每个消息保持多个状态（首先将其锁定，这样就不会再次发出该消息，然后将其标记为永久使用，以便可以将其删除）。必须处理棘手的问题，例如如何处理已发送但从未确认的消息。

`kafka`对此的处理方式有所不同。我们的主题分为一组完全有序的分区，每个分区都在任何给定时间由每个订阅的消费者组中的一个消费者正好使用。这意味着使用者在每个分区中的位置只是一个整数，即下一个要使用的消息的偏移量。这使得消耗的状态很小，每个分区只有一个数字。可以定期检查该状态。这使得消息确认的价格非常便宜。

这个决定有一个附带好处。消费者可以有意地倒回旧的偏移量并重新使用数据。这违反了队列的通用约定，但事实证明这是许多消费者的基本功能。例如，如果使用者代码有错误，并且在消费了某些消息后被发现，则消费者可以在错误修复后重新使用这些消息。

### 离线数据加载
可扩展的持久性允许消费者仅定期使用诸如批处理数据加载之类的数据，这些数据将周期性地将数据批量加载到诸如`Hadoop`或关系数据仓库的脱机系统中。
在`Hadoop`的情况下，我们通过将负载划分为各个地图任务来并行化数据负载，每个节点/主题/分区组合一个，从而实现了完全并行。`Hadoo`p提供了任务管理，失败的任务可以重新启动而没有重复数据的危险-它们只是从其原始位置重新启动。

### 静态成员
静态成员资格旨在提高流应用程序，消费者组和其他基于组重新平衡协议构建的应用程序的可用性。重新平衡协议依赖于组协调器将实体ID分配给组成员。这些生成的ID是短暂的，并且在成员重新启动并重新加入时将更改。对于基于消费者的应用程序，这种“动态成员身份”可以导致在管理操作（例如代码部署，配置更新和定期重新启动）期间将很大一部分任务重新分配给不同的实例。对于大型状态应用程序，混洗的任务需要很长时间才能恢复其本地状态，然后才能处理并导致应用程序部分或全部不可用。出于这一观察的动机，`Kafka`的群组管理协议允许群组成员提供持久的实体ID。根据这些ID，组成员身份保持不变，因此不会触发重新平衡。

如果您想使用静态成员资格，

- 将代理群集和客户端应用程序都升级到2.3或更高版本，并确保升级后的代理也使用`inter.broker.protocol.version 2.3`或更高版本。
- `ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`对于一个组中的每个使用者实例，将配置设置为唯一值。
- 对于`Kafka Streams`应用程序，为`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`每个`KafkaStreams`实例设置一个唯一值就足够了，而与该实例使用的线程数无关。

如果您的代理使用的版本低于2.3，但您选择`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`在客户端进行设置，则应用程序将检测到代理版本，然后引发`UnsupportedException`。如果您不小心为不同的实例配置了重复的ID，则代理端的防护机制会通过触发来通知您的重复客户端立即关闭`org.apache.kafka.common.errors.FencedInstanceIdException`。有关更多详细信息，请参见[`KIP-345`](https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances)

## 4.6 消息传递语义
现在我们对生产者和消费者的工作方式有了一些了解，让我们讨论`Kafka`在生产者和消费者之间提供的语义保证。显然，可以提供多种可能的消息传递保证：

- 最多一次-消息可能会丢失，但永远不会重新发送。
- 至少一次-消息永不丢失，但可以重新传递。
- 恰好一次 -人们真正想要的是，每条消息只传递一次，也只有一次。

值得注意的是，这可分为两个问题：发布消息的持久性保证和使用消息时的保证性。
许多系统声称提供“恰好一次”的交付语义，但是阅读精美印刷品很重要，这些声明中的大多数都具有误导性（即，它们不会转化为消费者或生产者可能失败，使用者程序，或写入磁盘的数据可能丢失的情况）。

`Kafka`的语义很简单。发布消息时，我们有一个消息被“提交”到日志的概念。提交已发布的消息后，只要复制此消息所写入分区的一个代理保持“活动”状态，它就不会丢失。下一节将更详细地描述已提交消息的定义，活动分区以及我们尝试处理的故障类型的描述。现在，让我们假设一个完美，无损的`broker`，并尝试了解对生产者和消费者的担保。如果生产者尝试发布消息并遇到网络错误，则无法确定此错误是在提交消息之前还是之后发生的。

在`0.11.0.0`之前，如果生产者未能收到表示已提交消息的响应，则除了重新发送消息外别无选择。这提供了至少一次的传递语义，因为如果原始请求实际上已经成功，则在重新发送期间可以将消息再次写入日志。从`0.11.0.0`开始，`Kafka`生产者还支持幂等交付选项，该选项保证重新发送不会导致日志中出现重复的条目。为实现此目的，代理为每个生产者分配一个`ID`，并使用生产者与每个消息一起发送的序列号对消息进行重复数据删除。同样从`0.11.0.0`开始，生产者支持使用类似于事务的语义将消息发送到多个主题分区的能力：即 要么所有消息都已成功写入，要么都不成功。主要用途是在`Kafka`主题之间进行一次准确的处理（如下所述）。

并非所有用例都需要如此有力的保证。对于对延迟敏感的用途，我们允许生产者指定所需的耐久性等级。如果生产者指定要等待提交的消息，则可能需要10毫秒左右的时间。但是，生产者也可以指定它想要完全异步执行发送，或者仅等待领导者（但不一定是跟随者）收到消息。

现在让我们从消费者的角度来描述语义。所有副本具有完全相同的日志和相同的偏移量。使用者控制其在此日志中的位置。如果使用方从未崩溃，则可以将该位置存储在内存中，但是如果使用方失败，并且我们希望此主题分区由另一个进程接管，则新进程将需要选择一个合适的位置来开始处理。假设消费者阅读了一些消息-它具有用于处理消息和更新其位置的多个选项。

它可以读取消息，然后将其位置保存在日志中，最后处理消息。在这种情况下，使用者进程有可能在保存其位置之后但在保存其消息处理的输出之前崩溃。在这种情况下，即使尚未处理该位置之前的一些消息，接管处理的过程也将从保存的位置开始。这对应于“最多一次”语义，因为在消费者失败消息的情况下，可能不会处理。
它可以读取消息，处理消息并最终保存其位置。在这种情况下，使用者进程可能在处理消息之后但在保存其位置之前崩溃。在这种情况下，当新进程接管收到的前几条消息时，将已经进行处理。在发生用户故障时，这对应于“至少一次”语义。在许多情况下，消息具有主键，因此更新是幂等的（两次接收同一条消息只是用另一个自身副本覆盖一条记录）。

那么，一次语义（即您真正想要的东西）又如何呢？从`Kafka`主题消费并产生另一个主题时（例如在`Kafka Streams`中） 应用程序，我们可以利用上面提到的`0.11.0.0`中的新事务生成器功能。消费者的位置作为消息存储在主题中，因此我们可以在与接收已处理数据的输出主题相同的事务中将偏移量写入`Kafka`。如果交易中止，则根据其他消费者的“隔离级别”，消费者的头寸将恢复为原来的值，并且输出主题中产生的数据对其他消费者而言是不可见的。在默认的`read_uncommitted`隔离级别中，所有消息对使用者都是可见的，即使它们是已中止的事务的一部分，但在`read_committed`中，使用者将仅从已提交的事务中返回消息（以及所有不属于事务的消息）交易）。

当写入外部系统时，限制在于需要协调消费者的位置和实际存储为输出的位置。实现此目的的经典方法是在消费者头寸的存储与消费者输出的存储之间引入两阶段提交。但这可以通过让消费者将其偏移量存储在与输出相同的位置来更简单地进行处理。这样做更好，因为使用者可能要写入的许多输出系统将不支持两阶段提交。例如，考虑一个`Kafka Connect`连接器，它在`HDFS`中填充数据以及它读取的数据的偏移量，从而确保数据和偏移量都被更新或都不更新。对于需要这些更强语义的其他许多数据系统，我们遵循类似的模式，对于这些数据，消息没有主键来允许重复数据删除。

因此，`Kafka`有效地支持`Kafka Streams`中的精确一次传递，并且在Kafka主题之间传输和处理数据时，事务性生产者/消费者通常可用于提供精确一次传递。通常仅向其他目标系统提供一次精确交付，但需要与此类系统合作，但是Kafka提供了补偿，这使得实现此目标变得可行（另请参见`Kafka Connect`）。否则，默认情况下，Kafka保证最少发送一次，并允许用户通过在处理一批消息之前禁用生产者的重试并在使用者中使用偏移来实现最多一次的传递。

## 4.7 复制
`Kafka`在可配置数量的服务器上复制每个主题分区的日志（您可以在逐个主题的基础上设置此复制因子）。当群集中的服务器发生故障时，这将允许自动故障转移到这些副本，因此在出现故障时消息仍然可用。

其他消息传递系统提供了一些与复制相关的功能，但是，根据我们的看法（完全有偏见），这似乎是一个固定的东西，没有被大量使用，并且具有很大的缺点：副本处于非活动状态，吞吐量受到严重影响，它需要默认情况下，`Kafka`打算与复制一起使用-实际上，我们将未复制的主题实现为复制主题，其中复制因子为1。

复制单元是主题分区。在非故障情况下，`Kafka`中的每个分区都有一个领导者和零个或多个跟随者。包括前导在内的副本总数构成复制因子。所有读取和写入都将转到分区的负责人。通常，分区比代理更多，并且领导者在代理之间平均分配。追随者上的日志与领导者的日志相同，所有日志都具有相同的偏移量和相同顺序的消息（当然，领导者在任何给定时间可能会在其日志末尾包含一些尚未复制的消息）。

追随者像普通的`Kafka`消费者一样，消费来自领导者的消息，并将其应用于自己的日志中。让追随者从领导者那里拉出具有很好的属性，即允许追随者自然地将他们要应用于其日志的日志条目一起批处理。

与大多数分布式系统一样，自动处理故障需要对节点“处于活动状态”的含义进行精确定义。对于`Kafka`节点，活跃度有两个条件

1. 节点必须能够与`ZooKeeper`保持会话（通过`ZooKeeper`的心跳机制）
2. 如果是追随者，则必须复制在领导者上发生的写入，并且不要落后太远

我们将满足这两个条件的节点称为“同步”，以避免模糊“有效”或“失败”。领导者跟踪“同步”节点的集合。如果追随者死亡，卡住或掉队，则领导者会将其从同步副本列表中删除。卡住和滞后的副本的确定是由`reploca.lag.time.max.ms`配置控制的。

在分布式系统术语中，我们仅尝试处理故障的“失败/恢复”模型，其中节点突然停止工作，然后又恢复（也许不知道它们已经死亡）。`Kafka`不处理所谓的“拜占庭式”故障，在这种故障中，节点会产生任意或恶意的响应（可能是由于错误或犯规行为引起的）。

现在，我们可以更精确地定义，当该分区的所有同步副本都将消息应用于其日志时，该消息就视为已提交。仅将已提交的消息发送给消费者。这意味着用户不必担心如果领导者失败可能会看到丢失的消息。另一方面，生产者可以根据等待时间和持久性之间的权衡选择，选择是否等待消息提交。此首选项由生产者使用的acks设置控制。请注意，主题具有用于同步副本的“最小数量”的设置，当生产者请求确认已将消息写入完整的同步副本集时，将检查该设置。

`Kafka`提供的保证是，只要始终有至少一个同步副本处于活动状态，提交的消息就不会丢失。

在短暂的故障转移期过后，如果出现节点故障，`Kafka`将仍然可用，但是在存在网络分区的情况下，`Kafka`可能仍然不可用。

### 复制的日志：仲裁，ISR和状态机（噢，我的天！）
从本质上讲，`Kafka`分区是复制日志。复制日志是分布式数据系统中最基本的原语之一，并且有许多实现方法。其他系统可以将复制的日志用作原语，以状态机样式实现其他分布式系统。
复制日志对一系列值的顺序达成共识的过程进行建模（通常对日志条目进行编号0、1、2，...）。有许多方法可以实现此目的，但最简单，最快的方法是由领导者选择提供给它的值的顺序。只要领导者还活着，所有关注者都只需要复制值并排序领导者即可选择。

当然，如果领导者没有失败，我们将不需要跟随者！领导者死后，我们需要从关注者中选择一位新领导者。但是追随者本身可能会落后或崩溃，因此我们必须确保我们选择最新的追随者。日志复制算法必须提供的基本保证是，如果我们告诉客户端消息已提交，而领导者失败，那么我们选择的新领导者也必须拥有该消息。这产生了一个折衷：如果领导者在宣布消息已落实之前等待更多的追随者确认一条消息，那么将会有更多潜在的可选举领导者。

如果您选择所需的确认数和选择领导者必须比较的日志数，以确保有重叠，那么这称为法定人数。

进行权衡的一种常见方法是对提交决定和领导人选举都使用多数票。这不是Kafka所做的，但是无论如何我们还是要探索一下它，以了解取舍。假设我们有`2f +1`个副本。如果`f+1`个副本必须在领导者声明提交之前收到消息，并且如果我们通过从至少`f+1`个副本中选择具有最完整日志的跟随者来选举新的领导者 ，则不超过`f`个如果失败，领导者将保证拥有所有已提交的消息。这是因为当中的任何`F+1`个副本，必须至少有一个包含所有已提交消息的副本。该副本的日志将是最完整的，因此将被选作新的领导者。每种算法都必须处理许多剩余的细节（例如，精确定义的内容使日志更完整，确保在领导者失败或更改副本集中的服务器集期间的日志一致性），但我们暂时将其忽略。

这种多数表决方法具有很好的特性：延迟仅取决于最快的服务器。即，如果复制因子为三，则延迟由较快的跟随者而不是较慢的跟随者确定。

还有在这个家庭的丰富多样的算法，包括`ZooKeeper`的`Zab`,`Raft`和`Viewstamped复制`。我们知道与`Kafka`的实际实施情况最相似的学术出版物是 M`icrosoft`的`PacificA`。

多数表决的不利之处在于，让您没有选举产生的领导人并不会导致很多失败。容忍一个故障需要数据的三个副本，而容忍两个故障则需要数据的五个副本。根据我们的经验，对于一个实际的系统而言，仅具有足够的冗余度来容忍单个故障是不够的，但是对于大容量数据问题而言，每次写入五次（磁盘空间需求为5倍，吞吐量为1/5）并不十分实际。这可能是为什么仲裁算法在共享群集配置（例如`ZooKeeper`）中更常见而在主数据存储中不常见的原因。例如，在`HDFS`中，`namenode`的高可用性功能是基于多数票的日记构建的，但是这种昂贵的方法并不用于数据本身。

`Kafka`采用稍微不同的方法来选择其仲裁集。`Kafka`无需进行多数投票，而是动态维护一组同步到领导者的同步副本（ISR）。只有这组成员才有资格当选领袖。直到所有异步副本都收到对`Kafka`分区的写入后，该写入才被视为已提交。每当更改时，此ISR集就会保留在`ZooKeeper`中。因此，ISR中的任何副本都有资格当选领导者。这是`kafka`使用模型的重要因素，因为该模型有很多分区，确保领导力平衡很重要。使用此`ISR`模型和`f+1`副本，`Kafka`主题可以容忍f个故障而不会丢失已落实的消息。

对于我们希望处理的大多数用例，我们认为这种权衡是合理的。在实践中，容忍˚F故障，多数表决和ISR方法将等待同样数量的副本提交的消息之前，承认两者（例如生存一次失败的多数仲裁需要三个副本和一个确认和ISR方法需要两个副本和一个确认）。在没有最慢服务器的情况下进行提交的能力是多数表决方法的优势。但是，我们认为可以通过允许客户端选择是否阻止消息提交来改善这种情况，并且由于所需的较低复制因子而增加的吞吐量和磁盘空间是值得的。

设计的另一个重要区别是，`Kafka`不需要崩溃的节点恢复其所有数据的完整性。在此空间中，复制算法依赖“稳定存储”的存在并不少见，这种稳定存储在任何故障恢复方案中都不会丢失，而不会潜在地违反一致性。这个假设有两个主要问题。首先，磁盘错误是我们在持久性数据系统的实际操作中观察到的最常见问题，它们通常不会使数据完整无缺。其次，即使这不是问题，我们也不希望在每次写入时都使用fsync来保证一致性，因为这会使性能降低2到3个数量级。我们的允许副本重新加入ISR的协议可确保在重新加入之前，

### 不干净的领导人选举：如果所有人都死了怎么办？
请注意，`Kafka`关于数据丢失的保证是基于至少一个保持同步的副本。如果所有复制分区的节点都死了，则此保证不再成立。
但是，当所有副本都消失时，实际系统需要做一些合理的事情。如果您很不幸发生这种情况，请务必考虑将要发生的事情。可以实现两种行为：

1. 等待`ISR`中的副本复活，然后选择该副本作为领导者（希望它仍然拥有所有数据）。
2. 选择第一个作为领导者复活的副本（不一定在`ISR`中）。

这是可用性和一致性之间的简单权衡。如果我们在`ISR`中等待副本，则只要这些副本已关闭，我们将保持不可用状态。如果此类副本被破坏或它们的数据丢失，那么我们将永远处于瘫痪状态。另一方面，如果非同步副本复活并且我们允许它成为领导者，那么即使不能保证它包含所有已提交的消息，其日志也将成为真相的来源。从`0.11.0.0`版本开始，默认情况下，`Kafka`选择第一个策略，并倾向于等待一致的副本。可以使用配置属性`unclean.leader.election.enable`更改此行为，以支持正常运行时间比一致性更好的用例。

这种困境并非仅针对`kafka`。它存在于任何基于仲裁的方案中。例如，在多数表决方案中，如果大多数服务器遭受永久性故障，那么您必须选择丢失100％的数据，或者通过将现有服务器上剩余的数据作为新的事实来源来破坏一致性。

### 可用性和耐用性保证
当写入`Kafka`时，生产者可以选择是等待消息被0,1还是全部（-1）副本确认。请注意，“所有副本的确认”不能保证已分配的副本的全部集合都已收到该消息。默认情况下，当`acks = all`时，所有当前的同步副本都收到消息后立即进行确认。例如，如果一个主题仅配置了两个副本，而一个失败（即仅保留一个同步副本），则指定`acks = all`的写入将成功。但是，如果其余副本也失败，则这些写操作可能会丢失。尽管这确保了分区的最大可用性，但是对于某些倾向于持久性而不是可用性的用户而言，此行为可能是不希望的。

1. 禁用不干净的领导者选择-如果所有副本都变得不可用，则该分区将保持不可用，直到最新的领导者再次变得可用为止。与消息丢失的风险相比，这实际上更倾向于不可用性。有关澄清，请参见上一节“不干净的领导人选举”。

2. 指定最小`ISR`大小-如果`ISR`的大小大于某个最小最小值，分区将仅接受写入，以防止丢失仅写入单个副本的消息，该消息随后将变得不可用。仅当生产者使用`acks = all`并保证至少有这么多同步副本确认该消息时，此设置才生效。此设置在一致性和可用性之间进行权衡。最小`ISR`大小的较高设置可确保更好的一致性，因为可以确保将消息写入更多副本，从而降低了丢失消息的可能性。但是，这会降低可用性，因为如果同步副本的数量下降到最小阈值以下，则分区将无法进行写操作。


### 副本管理
上面关于复制日志的讨论实际上只涵盖了一个日志，即一个主题分区。但是，`Kafka`群集将管理成百上千个这样的分区。我们尝试以循环方式平衡群集中的分区，以避免在少数节点上将所有分区聚集在一起，以应对大量主题。同样，我们尝试平衡领导力，以使每个节点都是按比例分配其分区的领导者。
优化领导层选举过程也很重要，因为这是不可用的关键窗口。最简单的领导者选举实现将最终在该节点发生故障时为该节点托管的所有分区的每个分区运行一个选举。相反，我们选择一名`broker`作为“控制人”。该控制器在代理级别检测到故障，并负责更改故障代理中所有受影响分区的领导者。结果是，我们能够将许多必需的领导层变更通知汇总在一起，从而使大量分区的选举过程便宜又快捷。如果控制者发生故障，幸存的`broker`之一将成为新的控制者。


## 4.8 日志压缩
日志压缩可确保`Kafka`将始终为单个主题分区的数据日志中的每个消息密钥至少保留最后一个已知值。它解决了一些用例和场景，例如在应用程序崩溃或系统故障后还原状态，或在运营维护期间在应用程序重启后重新加载缓存。让我们更详细地研究这些用例，然后描述压缩的工作方式。
到目前为止，我们仅描述了一种更简单的数据保留方法，该方法是在固定时间段后或日志达到某个预定大小后丢弃旧日志数据。这对于临时事件数据（例如，每个记录独立的日志记录）非常有效。但是，数据流的重要一类是对键控可变数据的更改的日志（例如，对数据库表的更改）。

让我们讨论这种流的具体示例。假设我们有一个包含用户电子邮件地址的主题；每次用户更新电子邮件地址时，我们都会使用其用户ID作为主键向该主题发送消息。现在说我们在一段时间内为ID为123的用户发送以下消息，每条消息都对应于电子邮件地址的更改（其他ID的消息被省略）：

```bash
123 => bill@microsoft.com
        .
        .
        .
123 => bill@gatesfoundation.org
        .
        .
        .
123 => bill@gmail.com
```
日志压缩为我们提供了更精细的保留机制，因此可以保证我们至少保留每个主键（例如`bill@gmail.com`）的最后一次更新。通过这样做，我们保证日志包含每个键的最终值的完整快照，而不仅仅是最近更改的键。这意味着下游使用者可以根据此主题恢复自己的状态，而无需保留所有更改的完整日志。
让我们先看一下一些有用的用例，然后看看如何使用它。

1. 数据库更改订阅。通常需要在多个数据系统中拥有一个数据集，并且这些系统中的一个经常是某种数据库（RDBMS或也许是新型的键值存储）。例如，您可能有一个数据库，一个缓存，一个搜索集群和一个`Hadoop`集群。对数据库的每次更改都需要反映在缓存，搜索集群中，并最终反映在`Hadoop`中。如果仅处理实时更新，则只需要最近的日志。但是，如果您希望能够重新加载缓存或还原失败的搜索节点，则可能需要完整的数据集。

2. 活动采购。这是一种应用程序设计风格，它将查询处理与应用程序设计共同定位，并使用更改日志作为应用程序的主要存储。

3. 日记功能以实现高可用性。通过注销对本地状态所做的更改，可以使执行本地计算的进程具有容错能力，以便另一个进程可以重新加载这些更改并在失败时继续执行。一个具体的例子是在流查询系统中处理计数，聚合和其他类似“分组”的处理。实时流处理框架`Samza`正是出于此目的使用此功能。

在每种情况下，主要需要实时处理更改，但是有时，当一台计算机崩溃或需要重新加载或重新处理数据时，则需要完成全部加载。日志压缩可以使这两个用例脱离同一个支持主题。此博客文章中更详细地描述了这种日志用法。

总体思路很简单。如果我们拥有无限的日志保留，并且在上述情况下记录了每次更改，那么我们将在系统首次启动时每次都捕获其状态。使用此完整的日志，我们可以通过重播日志中的前N条记录来恢复到任何时间点。对于多次更新单个记录的系统来说，这种假设的完整日志并不实用，因为即使对于稳定的数据集，日志也会无限制地增长。丢弃旧更新的简单日志保留机制将限制空间，但是日志不再是还原当前状态的方法-现在，从日志开始处恢复不再重新创建当前状态，因为可能根本无法捕获旧更新。

日志压缩是一种提供更细粒度的每个记录保留的机制，而不是提供基于时间的粗粒度保留的机制。这个想法是有选择地删除记录，其中我们使用相同的主键进行了更新。这样，可以确保日志至少具有每个键的最后状态。

可以按主题设置此保留策略，因此单个群集可以具有一些按大小或时间强制保留的主题，以及一些通过压缩强制保留的主题。

此功能的灵感来自`LinkedIn`最古老，最成功的基础架构之一，即名为`Databus`的数据库变更日志缓存服务。与大多数日志结构的存储系统不同，`Kafka`是为订阅而构建的，并组织数据以进行快速的线性读写。与`Databus`不同，`Kafka`充当真相源存储，因此即使在上游数据源无法以其他方式重播的情况下，它也很有用。

### 日志压缩基础
这是一个高级图片，显示了`Kafka`日志的逻辑结构以及每个消息的偏移量。

![](../images/log_cleaner_anatomy.png)

日志的头部与传统的`Kafka`日志相同。它具有密集的顺序偏移量，并保留所有消息。日志压缩增加了处理日志尾部的选项。上图显示了带有压紧尾巴的原木。请注意，日志尾部的消息保留首次写入时分配的原始偏移量-永远不会更改。还要注意，即使偏移量已压缩的消息，所有偏移量仍在日志中保持有效位置；在这种情况下，该位置与日志中出现的下一个最大偏移量是无法区分的。例如，在上方的图片中，偏移量36、37和38都是等效位置，从这些偏移量中的任何一个位置开始的读取操作都会返回以38开始的消息集。

压缩还允许删除。包含密钥和有效载荷为空的消息将被视为从日志中删除。此删除标记将导致删除所有具有该密钥的先前消息（与带有该密钥的任何新消息一样），但是删除标记的特殊之处在于，它们将在一段时间后从日志中清除，以释放空间。在上图中，不再保留删除的时间点标记为“删除保留点”。

通过定期重新复制日志段在后台完成压缩。清理不会阻止读取，可以限制清理以使用不超过可配置数量的`I/O`吞吐量，以避免影响生产者和消费者。压缩日志段的实际过程如下所示：

![](../images/log_compaction.png)

### 日志压缩提供什么保证？
日志压缩可确保以下各项：
1. 任何停留在日志开头的使用者都将看到所写的每条消息。这些消息将具有顺序的偏移量。`min.compaction.lag.ms`可以使用该主题来确保在编写消息之后必须经过的最短时间，然后才可以压缩消息。即，它为每个消息在（未压缩的）头部保留多长时间提供了下限。`max.compaction.lag.ms`可以使用该主题来确保在写入消息的时间与符合压缩条件的时间之间的最大延迟。
2. 消息的顺序始终保持不变。压缩将永远不会重新排序消息，仅删除一些消息即可。
3. 消息的偏移量永远不会改变。它是日志中某个位置的永久标识符。
4. 从日志开头开始的所有使用者都将至少按照其写入顺序看到所有记录的最终状态。此外，只要使用者在少于主题`delete.retention.ms`设置的时间范围内到达日志的开头（默认值为24小时），就会看到已删除记录的所有删除标记。换句话说：由于删除标记的删除与读取同时发生，因此如果消费者的删除标记滞后于，可能会错过删除标记`delete.retention.ms`。

### 日志压缩详细信息
日志压缩由日志清理器处理，日志清理器是后台线程池，用于重新复制日志段文件，并删除其键显示在日志开头的记录。每个压缩程序线程的工作方式如下：
1. 选择日志头与日志尾比最高的日志
2. 它为日志开头的每个键创建了最后一个偏移量的简要摘要
3. 它从头到尾重新复制日志，删除在日志中以后出现的键。新的干净段将立即交换到日志中，因此所需的额外磁盘空间只是一个额外的日志段（不是完整的日志副本）。
4. 日志头的摘要实际上只是一个紧凑的哈希表。每个条目恰好使用24个字节。结果，使用8GB的清理器缓冲区，一次清理器迭代可以清理大约366GB的日志头（假设有1000条消息）。

### 配置日志清理器
默认情况下，日志清除器处于启用状态。这将启动清理线程池。要启用特定主题的日志清理，请添加特定于日志的属性
```bash
log.cleanup.policy=compact
```
该`log.cleanup.policy`属性是在代理`server.properties`文件中定义的代理配置设置。它会影响群集中所有此处未配置替代的主题 。日志清理器可以配置为保留最小数量的未压缩的日志“头”。通过设置压缩时间滞后可以启用此功能。
```bash
log.cleaner.min.compaction.lag.ms
```
这可以用来防止比最小邮件使用期限新的邮件受到压缩。如果未设置，则除最后一个段（即当前正在写入的段）外，所有日志段都可以进行压缩。即使活动段的所有消息都早于最小压缩时间滞后，也不会对其进行压缩。日志清理器可以配置为确保最大延迟，在此之前，未压缩的日志“头”可以进行日志压缩。
```bash
log.cleaner.max.compaction.lag.ms
```
这可以用来防止生产率低下的原木无限制地压实一段持续的时间。如果未设置，则不压缩不超过`min.cleanable.dirty.ratio`的日志。请注意，此紧缩期限并不是硬性保证，因为它仍受日志清理程序线程的可用性和实际紧缩时间的约束。您将需要监视`uncleanable-partitions-count`，`max-clean-time-secs`和`max-compaction-delay-secs`指标。
此处 介绍了其他更清洁的配置。

## 4.9 配额
`Kafka`集群可以对请求强制执行配额，以控制客户端使用的代理资源。`kafka``broker`可以为共享配额的每组客户强制执行两种类型的客户配额：

1. 网络带宽配额定义字节速率阈值（从0.9开始）
2. 请求速率配额将CPU使用率阈值定义为网络和`I/O`线程的百分比（从0.11开始）

### 为什么需要配额？
生产者和消费者有可能生产/消费大量数据或以非常高的速率生成请求，从而垄断代理资源，导致网络饱和，并且通常导致DOS其他客户端和代理本身。拥有配额可以避免这些问题，并且在大型多租户群集中尤为重要，在该群集中，一小组行为​​不佳的客户端可能会降低行为良好的客户端的用户体验。实际上，当将`Kafka`作为服务运行时，甚至可以根据约定的合同强制执行`A​​PI`限制。

### 客户群
`Kafka`客户端的身份是用户主体，它代表安全集群中已通过身份验证的用户。在支持未经身份验证的客户端的群集中，用户主体是由代理使用可配置的方式选择的未经身份验证的用户的分组`PrincipalBuilder`。客户端ID是客户端的逻辑分组，其中客户端应用程序选择了有意义的名称。元组（用户，客户端ID）定义了共享用户主体和客户端ID的客户端安全逻辑组。
配额可以应用于（用户，客户端ID），用户或客户端ID组。对于给定的连接，将应用与该连接匹配的最具体的配额。配额组的所有连接共享为该组配置的配额。例如，如果（`user="test-user"，client-id="test-client"`）的生产配额为10MB/秒，则该配额在用户"test-user"的所有生产者实例中与客户端共享- id“测试客户端”。

### 配额配置
可以为（用户，客户端ID），用户和客户端ID组定义配额配置。可以在需要更高（甚至更低）配额的任何配额级别上覆盖默认配额。该机制类似于按主题的日志配置替代。用户和（用户，客户端ID）配额替代写入`/config/users`下的`ZooKeeper`，而客户端`ID`配额替代写入`/config/clients`下。所有代理均会读取这些替代，并立即生效。这使我们可以更改配额，而不必滚动重启整个集群。有关详细信息，请参见此处。每个组的默认配额也可以使用相同的机制动态更新。

配额配置的优先顺序为：

1. `/config/users/<user>/clients/<client-id>`
2. `/config/users/<user>/clients/<default>`
3. `/config/users/<user>`
4. `/config/users/<default>/clients/<client-id>`
5. `/config/users/<default>/clients/<default>`
6. `/config/users/<default>`
7. `/config/clients/<client-id>`
8. `/config/clients/<default>`

代理属性（`quota.producer.default，quota.consumer.default`）也可以用于为客户端ID组设置网络带宽配额的默认值。这些属性已被弃用，并将在以后的版本中删除。可以在`Zookeeper`中设置`client-id`的默认配额，类似于其他配额替代和默认配额。

### 网络带宽配额
网络带宽配额定义为共享配额的每组客户端的字节速率阈值。默认情况下，每个唯一的客户端组都会收到由群集配置的固定配额（以字节/秒为单位）。此配额是根据每个`broker`定义的。每个客户端组在限制客户端之前，每个代理最多可以发布/获取X个字节/秒。

### 请求率配额
请求速率配额定义为客户端可以在配额窗口内利用每个代理的请求处理程序`I/O`线程和网络线程的时间百分比。`n％`的配额表示 一个线程的`n％`，因此配额超出`((num.io.threads + num.network.threads)* 100)％`的总容量。每组客户可使用的总百分比最高为`n％`在配额窗口中跨所有`I/O`和网络线程访问。由于分配给`I/O`和网络线程的线程数通常基于代理主机上可用的内核数，因此请求率配额表示共享配额的每组客户端可以使用的`CPU`的总百分比。

### 执法
默认情况下，每个唯一客户端组都会收到由群集配置的固定配额。此配额是根据每个`broker`定义的。每个客户可以在限制每个`broker`之前使用此配额。我们决定为每个代理定义这些配额比为每个客户端具有固定的群集宽带带宽要好得多，因为这将需要一种在所有代理之间共享客户端配额使用情况的机制。比配额实现本身更难解决问题！

`broker`检测到配额违反情况时会如何反应？在我们的解决方案中，代理首先计算使违规客户端达到其配额所需的延迟量，并立即返回具有延迟的响应。在获取请求的情况下，响应将不包含任何数据。然后，代理将通向客户端的通道静音，不再处理来自客户端的请求，直到延迟结束。收到具有非零延迟持续时间的响应后，Kafka客户端还将避免在延迟期间向代理发送进一步的请求。因此，有效地阻止了来自客户端的请求。即使在较旧的客户端实现中，也不遵循代理的延迟响应，`broker`通过静音其套接字通道施加的背压仍然可以处理行为不佳的客户端的限制。向延迟通道发送了更多请求的那些客户端仅在延迟结束后才接收响应。

在多个小窗口（例如30个窗口，每个窗口1秒）上测量字节率和线程利用率，以便快速检测和纠正配额违规。通常，具有较大的测量窗口（例如，每个窗口有30秒的10个窗口）会导致大量的流量突发，并导致较长的延迟，这对于用户体验而言并不算太大。